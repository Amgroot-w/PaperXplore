"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Table of contents","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","C1","C1","Presents the table of contents for this issue of the publication.","1941-0026","","10.1109/TEVC.2020.3035481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273317","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"IEEE Transactions on Evolutionary Computation publication information","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","C2","C2","Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.","1941-0026","","10.1109/TEVC.2020.3035222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273279","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"Guest Editorial Special Issue on Theoretical Foundations of Evolutionary Computation","P. S. Oliveto; A. Auger; F. Chicano; C. M. Fonseca","Department of Computer Science, University of Sheffield, Sheffield, U.K.; INRIA, École Polytechnique IP Paris, Palaiseau, France; Department of Languages and Computing Sciences, University of Malaga, Málaga, Spain; Department of Informatics Engineering, CISUC, University of Coimbra, Coimbra, Portugal","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","993","994","It is our pleasure to introduce this special issue on the recent advances in the theoretical foundations of evolutionary computation (EC). While in the early days of this field, theoretical analyses inevitably focused on simplified models of evolutionary algorithms (EAs), the continuous progress made in the development of suitable mathematical techniques for the analysis now allows to derive proven statements regarding the performance of off-the-shelf metaheuristics, such as standard generational and steady-state genetic algorithms with no algorithmic simplifications. Comparisons between the performance of a given EA with the best possible one can also be made nowadays, allowing to assess whether a given algorithm may be improved upon or whether its performance is optimal for a given class of problems. Such understanding often provides insights for the design of new EAs which provably have better performance for given problems. We are glad that examples of results of this kind are present within this special issue. A total of 27 papers were submitted which were the subject of at least three independent reviews, and six manuscripts of the highest quality were selected for publication in the special issue. In the following, we provide a brief summary of these manuscripts.","1941-0026","","10.1109/TEVC.2020.3035225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273320","","","","","","","","0","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"A General Dichotomy of Evolutionary Algorithms on Monotone Functions","J. Lengler","Department of Computer Science, ETH Zürich, Zürich, Switzerland","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","995","1009","It is known that the (1 + 1)-EA with mutation rate c/n optimizes every monotone function efficiently if c <; 1, and needs exponential time on some monotone functions (HOTTOPIC functions) if c ≥ 2.2. We study the same question for a large variety of algorithms, particularly for the (1 + λ)-EA, (μ + 1)-EA, (μ + 1)-GA, their “fast” counterparts, and for the (1 + (λ, λ))GA. We find that all considered mutation-based algorithms show a similar dichotomy for HOTTOPIC functions, or even for all monotone functions. For the (1 + (λ, λ))-GA, this dichotomy is in the parameter cγ, which is the expected number of bit flips in an individual after mutation and crossover, neglecting selection. For the fast algorithms, the dichotomy is in m2/m1, where m1 and m2 are the first and second falling moment of the number of bit flips. Surprisingly, the range of efficient parameters is not affected by either population size μ nor by the offspring population size λ. The picture changes completely if crossover is allowed. The genetic algorithms (μ + 1)-GA and (μ+1)-fGA are efficient for arbitrary mutations strengths if μ is large enough.","1941-0026","","10.1109/TEVC.2019.2917014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715464","Computational and artificial intelligence;evolutionary computation;genetic algorithms","Runtime;Evolutionary computation;Genetic algorithms;Sociology;Statistics;Heuristic algorithms;Standards","genetic algorithms","evolutionary algorithms;monotone function;HOTTOPIC functions;mutation-based algorithms","","21","","32","IEEE","15 May 2019","","","IEEE","IEEE Journals"
"Parallel Black-Box Complexity With Tail Bounds","P. K. Lehre; D. Sudholt","School of Computer Science, University of Birmingham, Birmingham, U.K.; Department of Computer Science, University of Sheffield, Sheffield, U.K.","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1010","1024","We propose a new black-box complexity model for search algorithms evaluating λ search points in parallel. The parallel unary unbiased black-box complexity gives lower bounds on the number of function evaluations every parallel unary unbiased black-box algorithm needs to optimize a given problem. It captures the inertia caused by offspring populations in evolutionary algorithms and the total computational effort in parallel metaheuristics. We present complexity results for LeadingOnes and OneMax. Our main result is a general performance limit: we prove that on every function every λ-parallel unary unbiased algorithm needs at least a certain number of evaluations (a function of problem size and λ) to find any desired target set of up to exponential size, with an overwhelming probability. This yields lower bounds for the typical optimization time on unimodal and multimodal problems, for the time to find any local optimum, and for the time to even get close to any optimum. The power and versatility of this approach is shown for a wide range of illustrative problems from combinatorial optimization. Our performance limits can guide parameter choice and algorithm design; we demonstrate the latter by presenting an optimal λ-parallel algorithm for OneMax that uses parallelism most effectively.","1941-0026","","10.1109/TEVC.2019.2954234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8922641","Black-box complexity;parallelization;parameter control;runtime analysis;theory","Complexity theory;Optimization;Sociology;Statistics;Search problems;Evolutionary computation;Parallel processing","computational complexity;evolutionary computation;parallel algorithms;probability;search problems","parallel black-box complexity;tail bounds;black-box complexity model;search algorithms;λ search points;parallel unary unbiased black-box complexity;function evaluations;black-box algorithm;evolutionary algorithms;total computational effort;parallel metaheuristics;complexity results;general performance limit;typical optimization time;unimodal problems;multimodal problems;parameter choice;algorithm design;optimal λ-parallel algorithm;parallelism most effectively","","8","","64","CCBY","4 Dec 2019","","","IEEE","IEEE Journals"
"Significance-Based Estimation-of-Distribution Algorithms","B. Doerr; M. S. Krejca","Laboratoire d’Informatique, CNRS, École Polytechnique, Institute Polytechnique de Paris, Palaiseau, France; Hasso Plattner Institute, University of Potsdam, Potsdam, Germany","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1025","1034","Estimation-of-distribution algorithms (EDAs) are randomized search heuristics that create a probabilistic model of the solution space, which is updated iteratively, based on the quality of the solutions sampled according to the model. As previous works show, this iteration-based perspective can lead to erratic updates of the model, in particular, to bit-frequencies approaching a random boundary value. In order to overcome this problem, we propose a new EDA based on the classic compact genetic algorithm (cGA) that takes into account a longer history of samples and updates its model only with respect to information which it classifies as statistically significant. We prove that this significance-based cGA (sig-cGA) optimizes the commonly regarded benchmark functions OneMax (OM), LeadingOnes, and BinVal all in quasilinear time, a result shown for no other EDA or evolutionary algorithm so far. For the recently proposed stable compact genetic algorithm - an EDA that tries to prevent erratic model updates by imposing a bias to the uniformly distributed model - we prove that it optimizes OM only in a time exponential in its hypothetical population size. Similarly, we show that the convex search algorithm cannot optimize OM in polynomial time.","1941-0026","","10.1109/TEVC.2019.2956633","Investissement d’avenir Project, reference ANR-11-LABX-0056-LMH, LabEx LMH, in a joint call with Gaspard Monge Program for optimization, operations research and their interactions with data sciences; COST Action(grant numbers:CA15140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917722","Estimation-of-distribution algorithm (EDA);run time analysis;theory","Heuristic algorithms;Sociology;Statistics;History;Probabilistic logic;Benchmark testing;Genetic algorithms","computational complexity;distributed algorithms;evolutionary computation;genetic algorithms;probability;search problems","polynomial time;significance-based estimation-of-distribution;convex search algorithm;erratic model updates;compact genetic algorithm;evolutionary algorithm;OneMax;sig-cGA;significance-based cGA;classic compact genetic algorithm;random boundary value;probabilistic model;EDA","","17","","44","IEEE","28 Nov 2019","","","IEEE","IEEE Journals"
"Finite-Sample Analysis of Information Geometric Optimization With Isotropic Gaussian Distribution on Convex Quadratic Functions","K. Uchida; S. Shirakawa; Y. Akimoto","Graduate School of Environment and Information Sciences, Yokohama National University, Yokohama, Japan; Faculty of Environment and Information Sciences, Yokohama National University, Yokohama, Japan; Faculty of Engineering, Information and Systems, University of Tsukuba, Tsukuba, Japan","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1035","1049","We theoretically analyze the information geometric optimization (IGO), which is a unified framework of stochastic search algorithms for black-box optimization. The IGO framework has two parameters: 1) the learning rate and 2) the sample size, and they influence the behavior of the algorithm. We investigate the strategy parameters of the IGO with the family of isotropic Gaussian distributions on a general convex quadratic function. Compared to the previous theoretical works, where an infinite sample size is assumed and the deterministic algorithm dynamics is studied, we investigate the expected improvement of the algorithm with a finite sample size. The analysis finds that the relative decrease rates of the distance from the distribution mean to the landscape optimum and the distribution standard deviation must be the same, which we observe in practice, while the analysis based on an infinite sample size failed to obtain. We derive these rates explicitly as a function of the eigenvalues of the Hessian of the objective function and the strategy parameters. We also derive the stable value of the ratio of the square distance to the optimum over the distribution variance, as well as the conditions that the stable value exists. These theoretical values coincide with our numerical simulations.","1941-0026","","10.1109/TEVC.2019.2917709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718813","Convex quadratic function;finite-sample analysis;information geometric optimization (IGO);learning rates;single step expected improvement","Optimization;Gaussian distribution;Heuristic algorithms;Covariance matrices;Standards;Linear programming;Stochastic processes","convex programming;deterministic algorithms;eigenvalues and eigenfunctions;Gaussian distribution;Hessian matrices;learning (artificial intelligence);search problems;stochastic processes","finite-sample analysis;information geometric optimization;isotropic Gaussian distribution;stochastic search algorithms;black-box optimization;IGO;learning rate;convex quadratic function;infinite sample size;deterministic algorithm dynamics;finite sample size;relative decrease rates;distribution mean;distribution standard deviation;objective function;distribution variance;eigenvalues;Hessian;square distance","","4","","25","IEEE","21 May 2019","","","IEEE","IEEE Journals"
"Population Diversity of Nonelitist Evolutionary Algorithms in the Exploration Phase","J. Arabas; K. Opara","Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland; Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1050","1062","This paper discusses the genetic diversity of real-coded populations processed by an evolutionary algorithm (EA). Diversity is expressed as a variance or a covariance matrix of individuals contained in the population, in one- or multi-dimensional cases, respectively. We focus on the exploration stage of the optimization, therefore, the fitness function is modeled as noise. We prove that the expected value of genetic diversity achieves a level proportional to the mutation covariance matrix. The proportionality coefficient depends solely on the EA parameters. Formulas are derived to predict the diversity for fitness proportionate, tournament, and truncation selection, with and without arithmetic crossover and with Gaussian mutation. Experimental validation of the multidimensional case shows that prediction accuracy is satisfactory in a broad spectrum of settings of EA parameters.","1941-0026","","10.1109/TEVC.2019.2917275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716593","Evolutionary algorithm (EA);noise fitness model;population diversity","Sociology;Genetics;Correlation;Covariance matrices;Dispersion;Random variables","covariance matrices;evolutionary computation;genetic algorithms","population diversity;nonelitist evolutionary algorithms;exploration phase;genetic diversity;real-coded populations;evolutionary algorithm;multidimensional cases;exploration stage;fitness function;mutation covariance matrix;proportionality coefficient;EA parameters;fitness proportionate;multidimensional case","","10","","23","IEEE","16 May 2019","","","IEEE","IEEE Journals"
"Landscape-Aware Performance Prediction for Evolutionary Multiobjective Optimization","A. Liefooghe; F. Daolio; S. Verel; B. Derbel; H. Aguirre; K. Tanaka","CNRS, Centrale Lille, UMR 9189 – CRIStAL, Inria Lille–Nord Europe, University of Lille, Lille, France; ASOS.com, AI Platform, London, U.K.; LISIC, Université Littoral Côte d’Opale, Calais, France; CNRS, Centrale Lille, UMR 9189 – CRIStAL, Inria Lille–Nord Europe, University of Lille, Lille, France; Faculty of Engineering, Shinshu University, Nagano, Japan; Faculty of Engineering, Shinshu University, Nagano, Japan","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1063","1077","We expose and contrast the impact of landscape characteristics on the performance of search heuristics for black-box multiobjective combinatorial optimization problems. A sound and concise summary of features characterizing the structure of an arbitrary problem instance is identified and related to the expected performance of global and local dominance-based multiobjective optimization algorithms. We provide a critical review of existing features tailored to multiobjective combinatorial optimization problems, and we propose additional ones that do not require any global knowledge from the landscape, making them suitable for large-size problem instances. Their intercorrelation and their association with algorithm performance are also analyzed. This allows us to assess the individual and the joint effect of problem features on algorithm performance, and to highlight the main difficulties encountered by such search heuristics. By providing effective tools for multiobjective landscape analysis, we highlight that multiple features are required to capture problem difficulty, and we provide further insights into the importance of ruggedness and multimodality to characterize multiobjective combinatorial landscapes.","1941-0026","","10.1109/TEVC.2019.2940828","Japan Society for the Promotion of Science (JSPS) Strategic Program “Global Research on the Framework of Evolutionary Solution Search to Accelerate Innovation” (2013–2016); JSPS-Inria Project “Threefold Scalability in Any-Objective Black-Box Optimization” (2015–2017); French National Research Agency Project “Big Multiobjective Optimization” (2017–2021)(grant numbers:ANR-16-CE23-0013-01); CNRS/FCT Project “MOCO-SEARCH” (2018–2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832171","Black-box combinatorial optimization;evolutionary multiobjective optimization (EMO);feature-based performance prediction;problem difficulty and landscape analysis","Optimization;Prediction algorithms;Search problems;Correlation;Machine learning algorithms;Face;Linear programming","combinatorial mathematics;evolutionary computation;optimisation;search problems","algorithm performance;search heuristics;multiobjective landscape analysis;multiobjective combinatorial landscapes;landscape-aware performance prediction;evolutionary multiobjective optimization;landscape characteristics;arbitrary problem instance;local dominance-based multiobjective optimization;global dominance-based multiobjective optimization;black-box multiobjective combinatorial optimization","","32","","55","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"Does Preference Always Help? A Holistic Study on Preference-Based Evolutionary Multiobjective Optimization Using Reference Points","K. Li; M. Liao; K. Deb; G. Min; X. Yao","Department of Computer Science, University of Exeter, Exeter, U.K.; College of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-Inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1078","1096","The ultimate goal of multiobjective optimization is to help a decision maker (DM) identify solution(s) of interest (SOI) achieving satisfactory tradeoffs among multiple conflicting criteria. This can be realized by leveraging DM's preference information in evolutionary multiobjective optimization (EMO). No consensus has been reached on the effectiveness brought by incorporating preference in EMO (either a priori or interactively) versus a posteriori decision making after a complete run of an EMO algorithm. Bearing this consideration in mind, this article: 1) provides a pragmatic overview of the existing developments of preference-based EMO (PBEMO) and 2) conducts a series of experiments to investigate the effectiveness brought by preference incorporation in EMO for approximating various SOI. In particular, the DM's preference information is elicited as a reference point, which represents her/his aspirations for different objectives. The experimental results demonstrate that preference incorporation in EMO does not always lead to a desirable approximation of SOI if the DM's preference information is not well utilized, nor does the DM elicit invalid preference information, which is not uncommon when encountering a black-box system. To a certain extent, this issue can be remedied through an interactive preference elicitation. Last but not the least, we find that a PBEMO algorithm is able to be generalized to approximate the whole PF given an appropriate setup of preference information.","1941-0026","","10.1109/TEVC.2020.2987559","UKRI Future Leaders Fellowship(grant numbers:MR/S017062/1); Royal Society(grant numbers:IEC/NSFC/170243); Guangdong Provincial Key Laboratory of Brain-Inspired Intelligent Computation; Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X386); Shenzhen Science and Technology Program(grant numbers:KQTD2016112514355531); Program for University Key Laboratory of Guangdong Province(grant numbers:2017KSYS008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066927","Decision-making;evolutionary multiobjective optimization (EMO);preference incorporation;reference point","Approximation algorithms;Optimization;Decision making;Computer science;Electronic mail;Zirconium;Linear programming","decision making;evolutionary computation;Pareto optimisation","EMO algorithm;preference-based EMO;preference incorporation;SOI;DM's preference information;reference point;DM elicit invalid preference information;interactive preference elicitation;preference-based evolutionary multiobjective optimization;PBEMO algorithm","","27","","87","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Empirical Linkage Learning","M. W. Przewozniczek; M. M. Komarnicki","Department of Computational Intelligence, Wroclaw University of Science and Technology, Wrocław, Poland; Department of Computational Intelligence, Wroclaw University of Science and Technology, Wrocław, Poland","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1097","1111","Linkage learning techniques are a crucial part of many modern evolutionary methods dedicated to solving problems in discrete domains. Linkage information quality is decisive for the effectiveness of these methods. In this article, we point on two possible linkage inaccuracy types. The missing linkage that occurs when some gene dependencies remain undiscovered, and the false linkage that takes place when linkage identifies gene dependencies that do not exist. To the best of our knowledge, all linkage learning techniques proposed so far are based on predictions, which can commit both of the mistake types. We propose a different approach. Instead of using statistical measures, or evolving the linkage, we check which genes are dependent on one another employing disturbances and the local search. We prove that the proposed technique will never report any false linkage. Thus, the proposed linkage learning based on local optimization (3LO) may miss some linkage but will never report a false one. The main objective of this article is to show the potential brought by 3LO that is fundamentally different from other linkage learning techniques. Since the main disadvantage of the proposed technique is its computational cost, it does not seem suitable for some of the already known, effective evolutionary methods. To overcome this issue, we propose an evolutionary method that employs 3LO. The extensive experimental analysis performed on a large set of hard computational problems shows that the method using 3LO is found to be competitive with other state-of-the-art methods.","1941-0026","","10.1109/TEVC.2020.2985497","Polish National Science Centre (NCN)(grant numbers:2015/19/D/ST6/03115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9056803","Genetic algorithm;linkage learning;parameter-less","Couplings;Sociology;Statistics;Optimization;Genetic algorithms;Perturbation methods;Evolutionary computation","genetic algorithms;learning (artificial intelligence);search problems","3LO;linkage learning techniques;evolutionary methods;empirical linkage learning;linkage information quality;linkage learning based on local optimization","","16","","22","IEEE","3 Apr 2020","","","IEEE","IEEE Journals"
"A Hybrid Deep Grouping Algorithm for Large Scale Global Optimization","H. Liu; Y. Wang; N. Fan","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1112","1124","Many real-world problems contain a large number of decision variables which can be modeled as large scale global optimization (LSGO) problems. One effective way to solve an LSGO problem is to decompose it into smaller subproblems to solve. The existing works mainly focused on designing methods to decompose separable problems, while seldom focused on the decomposition of nonseparable large scale problems. Also, the existing decomposition methods only learn the interaction (correlation or interdependence) among variables to make the decomposition. In this article, we make the decomposition deeper: we not only consider the variable interaction but also take the essentialness of the variable into account to form a deep grouping method. To do this, we first design an essential/trivial variable detection scheme to support the deep decomposition for both separable problems and nonseparable problems. Based on it, we propose a new decomposition method called deep grouping method. Then, we design a new differential evolution (DE) algorithm with a new mutation strategy. By integrating all these, we propose a hybrid deep grouping (HDG) algorithm. Finally, the experiments are conducted on the widely used and most challenging LSGO benchmark suites, and the comparison results of the proposed algorithm with the state-of-the-art algorithms indicate the proposed algorithm is more effective.","1941-0026","","10.1109/TEVC.2020.2985672","National Natural Science Foundation of China(grant numbers:61872281); Key Natural Science Foundation of Shaanxi Province(grant numbers:2016JZ022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057540","Decomposition (grouping) strategy;deep grouping;large scale optimization;problem decomposition","Optimization;Correlation;Computer science;Fans;Benchmark testing;Search problems;Entropy","evolutionary computation;optimisation","hybrid deep grouping algorithm;decision variables;large scale global optimization problems;separable problems;nonseparable large scale problems;variable interaction;deep grouping method;deep decomposition;nonseparable problems;decomposition method;LSGO benchmark suites","","36","","46","IEEE","6 Apr 2020","","","IEEE","IEEE Journals"
"Binary Relation Learning and Classifying for Preselection in Evolutionary Algorithms","H. Hao; J. Zhang; X. Lu; A. Zhou","Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Southern University of Science and Technology, Shenzhen, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1125","1139","Evolutionary algorithms (EAs) are a kind of population-based heuristic optimization method by using trial-and-error. Therefore, the search efficiency is a major concern in both of the algorithm design and applications. The preselection, which estimates the quality of candidate solutions and discards unpromising ones before fitness evaluation, is a widely used component for reducing the number of fitness evaluations in EAs. The surrogate models, such as regression and classification, are usually applied for quality estimation. In some EA frameworks, the relationship between a pair of solutions helps to distinguish “good” and “bad” solutions. In such cases, it is not necessary to estimate the specific quality of each candidate solution but the binary relationship of a pair of solutions. Following this idea, this article proposes a new preselection strategy, called relationship classification-based preselection (RCPS). In RCPS, a classification model is built to learn the relationship between a pair of solutions based on a given training data set, and promising candidate solutions are prescreened by this relation. The mechanism of RCPS is visualized and analyzed. The advantages of RCPS over traditional surrogate model-based preselection strategies are illustrated through a comprehensive empirical study. The experimental results suggest that on two sets of test suits, RCPS outperforms the comparison preselection strategies. To achieve a same accuracy, an EA with RCPS needs a smaller number of fitness evaluations than the one without RCPS.","1941-0026","","10.1109/TEVC.2020.2986348","Science and Technology Commission of Shanghai Municipality(grant numbers:19511120600); National Nature Science Foundation of China(grant numbers:61773296,61673180); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060983","Classification;evolutionary algorithm (EA);preselection;regression;surrogate model","Sociology;Statistics;Training data;Training;Data models;Linear programming;Classification algorithms","estimation theory;evolutionary computation;learning (artificial intelligence);optimisation;pattern classification","RCPS;evolutionary algorithms;population-based heuristic optimization;trial-and-error;surrogate models;quality estimation;EA frameworks;binary relation learning;relationship classification-based preselection","","12","","58","IEEE","8 Apr 2020","","","IEEE","IEEE Journals"
"Sharp Bounds for Genetic Drift in Estimation of Distribution Algorithms","B. Doerr; W. Zheng","Laboratoire d’Informatique, CNRS, École Polytechnique, Institut Polytechnique de Paris, Palaiseau, France; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1140","1149","Estimation of distribution algorithms (EDAs) are a successful branch of evolutionary algorithms (EAs) that evolve a probabilistic model instead of a population. Analogous to genetic drift in EAs, EDAs also encounter the phenomenon that the random sampling in the model update can move the sampling frequencies to boundary values not justified by the fitness. This can result in a considerable performance loss. This article gives the first tight quantification of this effect for three EDAs and one ant colony optimizer, namely, for the univariate marginal distribution algorithm, the compact genetic algorithm, population-based incremental learning, and the max-min ant system with iteration-best update. Our results allow to choose the parameters of these algorithms in such a way that within a desired runtime, no sampling frequency approaches the boundary values without a clear indication from the objective function.","1941-0026","","10.1109/TEVC.2020.2987361","Investissement d’Avenir Project, LabEx LMH, in a joint call with Gaspard Monge Program for optimization, operations research and their interactions with data sciences(grant numbers:ANR-11-LABX-0056-LMH); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X386); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2019A1515110177); Shenzhen Peacock Plan(grant numbers:KQTD2016112514355531); Program for University Key Laboratory of Guangdong Province(grant numbers:2017KSYS008); Science and Technology Innovation Committee Foundation of Shenzhen(grant numbers:JCYJ20190809121403553); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064720","Estimation of distribution algorithms (EDAs);genetic drift;running time analysis;theory","Sociology;Statistics;Probabilistic logic;Genetics;Computational modeling;Time-frequency analysis;Genetic algorithms","ant colony optimisation;estimation theory;genetic algorithms;learning (artificial intelligence);minimax techniques;search problems","max-min ant system;population-based incremental learning;compact genetic algorithm;univariate marginal distribution algorithm;ant colony optimizer;performance loss;boundary values;sampling frequency;model update;random sampling;probabilistic model;EAs;evolutionary algorithms;EDAs;distribution algorithms","","21","","27","IEEE","13 Apr 2020","","","IEEE","IEEE Journals"
"A Framework for Scalable Bilevel Optimization: Identifying and Utilizing the Interactions Between Upper-Level and Lower-Level Variables","P. -Q. Huang; Y. Wang","School of Automation, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1150","1163","When solving bilevel optimization problems (BOPs) by evolutionary algorithms (EAs), it is necessary to obtain the lower-level optimal solution for each upper-level solution, which gives rise to a large number of lower-level fitness evaluations, especially for large-scale BOPs. It is interesting to note that some upper-level variables may not interact with some lower-level variables. Under this condition, if the value(s) of one/several upper-level variables change(s), we only need to focus on the optimization of the interacting lower-level variables, thus reducing the dimension of the search space and saving the number of lower-level fitness evaluations. This article proposes a new framework (called GO) to identify and utilize the interactions between upper-level and lower-level variables for scalable BOPs. GO includes two phases: 1) the grouping phase and 2) the optimization phase. In the grouping phase, after identifying the interactions between upper-level and lower-level variables, they are divided into three types of subgroups (denoted as types I-III), which contain only upper-level variables, only lower-level variables, and both upper-level and lower-level variables, respectively. In the optimization phase, if type-I and type-II subgroups only include one variable, a multistart sequential quadratic programming is designed; otherwise, a single-level EA is applied. In addition, a criterion is proposed to judge whether a type-II subgroup has multiple optima. If multiple optima exist, by incorporating the information of the upper level, we design new objective function and degree of constraint violation to locate the optimistic solution. As for type-III subgroups, they are optimized by a bilevel EA (BLEA). The effectiveness of GO is demonstrated on a set of scalable test problems by applying it to five representative BLEAs. Moreover, GO is applied to the resource pricing in mobile edge computing.","1941-0026","","10.1109/TEVC.2020.2987804","Innovation-Driven Plan in Central South University(grant numbers:2018CX010); National Natural Science Foundation of China(grant numbers:61673397,61976225); Beijing Advanced Innovation Center for Intelligent Robots and Systems(grant numbers:2018IRS06); Foundational Research Funds for the Central Universities of Central South University(grant numbers:2020zzts521); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067069","Bilevel optimization;evolutionary algorithms (EAs);interaction;lower-level variables;mobile edge computing (MEC);resource pricing;upper-level variables","Optimization;Linear programming;Iron;Evolutionary computation;Pricing;Edge computing;Computational modeling","genetic algorithms;group theory;quadratic programming;search problems","bilevel EA;multistart sequential quadratic programming;grouping phase;search space dimension;evolutionary algorithms;bilevel optimization problems;lower level variables;upper level variables","","20","","55","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Characterizing Genetic Programming Error Through Extended Bias and Variance Decomposition","C. A. Owen; G. Dick; P. A. Whigham","Department of Information Science, University of Otago, Dunedin, New Zealand; Department of Information Science, University of Otago, Dunedin, New Zealand; Department of Information Science, University of Otago, Dunedin, New Zealand","IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1164","1176","An error function can be used to select between candidate models but it does not provide a thorough understanding of the behavior of a model. A greater understanding of an algorithm can be obtained by performing a bias-variance decomposition. Splitting the error into bias and variance is effective for understanding a deterministic algorithm such as k-nearest neighbor, which provides the same predictions when performed multiple times using the same data. However, simply splitting the error into bias and variance is not sufficient for nondeterministic algorithms, such as genetic programming (GP), which potentially produces a different model each time it is run, even when using the same data. This article presents an extended bias-variance decomposition that decomposes error into bias, external variance (error attributable to limited sampling of the problem), and internal variance (error due to random actions performed in the algorithm itself). This decomposition is applied to GP to expose the three components of error, providing a unique insight into the role of maximum tree depth, number of generations, size/complexity of function set, and data standardization in influencing predictive performance. The proposed tool can be used to inform targeted improvements for reducing specific components of model error.","1941-0026","","10.1109/TEVC.2020.2990626","University of Otago Doctoral Scholarship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080104","Bias-variance decomposition;bias-variance tradeoff;evolutionary machine learning (EML);genetic programming (GP);prediction error;symbolic regression","Predictive models;Prediction algorithms;Machine learning;Data models;Machine learning algorithms;Genetic programming;Stochastic processes","genetic algorithms;statistical analysis;trees (mathematics)","internal variance;GP;data standardization;error function;candidate models;deterministic algorithm;k-nearest neighbor;nondeterministic algorithms;bias-variance decomposition;genetic programming error","","4","","33","IEEE","28 Apr 2020","","","IEEE","IEEE Journals"
"IEEE Congress on Evolutionary Computation Call for Papers","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1177","1177","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","1941-0026","","10.1109/TEVC.2020.3026458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273286","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"TechRxiv: Share Your Preprint Research with the World!","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1178","1178","Advertisement: TechRxiv is a free preprint server for unpublished research in electrical engineering, computer science, and related technology. TechRxiv provides researchers the opportunity to share early results of their work ahead of formal peer review and publication. Benefits: Rapidly disseminate your research findings; Gather feedback from fellow researchers; Find potential collaborators in the scientific community; Establish the precedence of a discovery; and Document research results in advance of publication. Upload your unpublished research today!","1941-0026","","10.1109/TEVC.2020.3036176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273278","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"Introducing IEEE Collabratec","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1179","1179","Advertisement, IEEE. IEEE Collabratec is a new, integrated online community where IEEE members, researchers, authors, and technology professionals with similar fields of interest can network and collaborate, as well as create and manage content. Featuring a suite of powerful online networking and collaboration tools, IEEE Collabratec allows you to connect according to geographic location, technical interests, or career pursuits. You can also create and share a professional identity that showcases key accomplishments and participate in groups focused around mutual interests, actively learning from and contributing to knowledgeable communities. All in one place! Learn about IEEE Collabratec at ieeecollabratec.org.","1941-0026","","10.1109/TEVC.2020.3036177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273277","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"IEEE Access","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","1180","1180","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","1941-0026","","10.1109/TEVC.2020.3036178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273316","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"IEEE Transactions on Evolutionary Computation Society Information","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","C3","C3","Presents a listing of the editorial board, board of governors, current staff, committee members, and/or society editors for this issue of the publication.","1941-0026","","10.1109/TEVC.2020.3035223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273287","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
"IEEE Transactions on Evolutionary Computation information for authors","",,"IEEE Transactions on Evolutionary Computation","30 Nov 2020","2020","24","6","C4","C4","These instructions give guidelines for preparing papers for this publication. Presents information for authors publishing in this journal.","1941-0026","","10.1109/TEVC.2020.3035224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273311","","","","","","","","","IEEE","30 Nov 2020","","","IEEE","IEEE Journals"
